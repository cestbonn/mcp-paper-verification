"""
ç»¼åˆè®ºæ–‡éªŒè¯å™¨
åŒ…å«å¤šç§éªŒè¯åŠŸèƒ½ï¼šç½—åˆ—æ£€æŸ¥ã€åˆ»æ¿å°è±¡æ£€æŸ¥ã€æ ¼å¼æ£€æŸ¥ã€å¼•ç”¨æ£€æŸ¥ç­‰
"""

import os
import re
import json
import statistics
import http.client
import urllib.parse
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple
import bibtexparser


class WebSearchAPI:
    """Google Serper APIå®¢æˆ·ç«¯"""
    
    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key or os.getenv('SERPER_API_KEY')
        self.base_url = "google.serper.dev"
        
    def search_reference(self, title: str, authors: str = "") -> Dict[str, Any]:
        """æœç´¢å‚è€ƒæ–‡çŒ®æ˜¯å¦çœŸå®å­˜åœ¨"""
        if not self.api_key:
            return {
                "success": False,
                "error": "SERPER_API_KEY not provided"
            }
        
        # æ„å»ºæœç´¢æŸ¥è¯¢
        query = f'"{title}"'
        if authors:
            query += f' {authors}'
        
        payload = {
            "q": query,
            "num": 3
        }
        
        try:
            conn = http.client.HTTPSConnection(self.base_url)
            headers = {
                'X-API-KEY': self.api_key,
                'Content-Type': 'application/json'
            }
            
            conn.request("POST", "/search", json.dumps(payload), headers)
            response = conn.getresponse()
            data = response.read()
            
            if response.status != 200:
                return {
                    "success": False,
                    "error": f"API request failed with status {response.status}"
                }
            
            result = json.loads(data.decode("utf-8"))
            
            # æ£€æŸ¥æ˜¯å¦æ‰¾åˆ°ç›¸å…³ç»“æœ
            if "organic" in result and len(result["organic"]) > 0:
                return {
                    "success": True,
                    "found": True,
                    "results": result["organic"][:3]
                }
            else:
                return {
                    "success": True,
                    "found": False,
                    "results": []
                }
                
        except Exception as e:
            return {
                "success": False,
                "error": f"Search request failed: {str(e)}"
            }
        finally:
            if 'conn' in locals():
                conn.close()


class PaperVerifier:
    """ç»¼åˆè®ºæ–‡éªŒè¯å™¨"""
    
    def __init__(self, serper_api_key: Optional[str] = None):
        self.web_search = WebSearchAPI(serper_api_key)
    
    def verify_sparse_content(self, content: str) -> Dict[str, Any]:
        """æ£€æŸ¥ç½—åˆ—æƒ…å†µï¼ˆåŸºäºsparse_verifier.pyï¼‰"""
        
        # åŸºæœ¬ç»Ÿè®¡
        total_chars = len(content)
        total_chars_no_spaces = len(content.replace(' ', '').replace('\t', '').replace('\n', ''))
        
        # åˆ†å‰²æ®µè½
        paragraphs = []
        sections = re.split(r'\n\s*\n', content.strip())
        
        for section in sections:
            if section.strip():
                lines = section.split('\n')
                current_para = []
                
                for line in lines:
                    line = line.strip()
                    if line:
                        # è·³è¿‡markdownæ ‡é¢˜è¡Œ
                        if re.match(r'^#{1,6}\s+', line):
                            if current_para:
                                paragraphs.append('\n'.join(current_para))
                                current_para = []
                            continue
                        current_para.append(line)
                    elif current_para:
                        paragraphs.append('\n'.join(current_para))
                        current_para = []
                
                if current_para:
                    paragraphs.append('\n'.join(current_para))
        
        # è¿‡æ»¤ç©ºæ®µè½å’Œè¿‡çŸ­æ®µè½
        paragraphs = [p.strip() for p in paragraphs if p.strip() and len(p.strip()) > 20]
        
        if not paragraphs:
            return {
                "has_issues": True,
                "issues": ["No meaningful paragraphs found"],
                "sparsity_score": 1.0
            }
        
        # è®¡ç®—æ®µè½é•¿åº¦ç»Ÿè®¡
        paragraph_lengths = [len(p) for p in paragraphs]
        median_length = statistics.median(paragraph_lengths)
        mean_length = statistics.mean(paragraph_lengths)
        
        issues = []
        sparsity_score = 0.0
        
        # 1. çŸ­æ®µè½æ¯”ä¾‹æ£€æŸ¥
        short_paragraphs = [p for p in paragraph_lengths if p < 300]
        short_para_ratio = len(short_paragraphs) / len(paragraph_lengths)
        if short_para_ratio > 0.6:
            issues.append(f"è¿‡å¤šçŸ­æ®µè½ ({short_para_ratio:.1%} çš„æ®µè½å°‘äº300å­—ç¬¦)")
            sparsity_score += 0.3
        
        # 2. æçŸ­æ®µè½æ¯”ä¾‹æ£€æŸ¥
        very_short_paragraphs = [p for p in paragraph_lengths if p < 100]
        very_short_ratio = len(very_short_paragraphs) / len(paragraph_lengths)
        if very_short_ratio > 0.4:
            issues.append(f"è¿‡å¤šæçŸ­æ®µè½ ({very_short_ratio:.1%} çš„æ®µè½å°‘äº100å­—ç¬¦)")
            sparsity_score += 0.2
        
        # 3. æ£€æµ‹åˆ—è¡¨æ ‡è®°æ¨¡å¼
        list_patterns = [
            r'^\s*\d+[\.\)]\s*',  # æ•°å­—åˆ—è¡¨
            r'^\s*[a-zA-Z][\.\)]\s*',  # å­—æ¯åˆ—è¡¨
            r'^\s*[-\*\+â€¢]\s*',  # ç¬¦å·åˆ—è¡¨
            r'^\s*[ç¬¬]\d+[ç« èŠ‚æ¡]\s*',  # ä¸­æ–‡ç« èŠ‚æ ‡è®°
            r'^\s*[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[\.\ã€]\s*',  # ä¸­æ–‡æ•°å­—åˆ—è¡¨
        ]
        
        list_like_paragraphs = 0
        for para in paragraphs:
            for pattern in list_patterns:
                if re.search(pattern, para, re.MULTILINE):
                    list_like_paragraphs += 1
                    break
        
        list_ratio = list_like_paragraphs / len(paragraphs)
        if list_ratio > 0.3:
            issues.append(f"è¿‡å¤šåˆ—è¡¨å¼æ®µè½ ({list_ratio:.1%} çš„æ®µè½ä¸ºåˆ—è¡¨æ ¼å¼)")
            sparsity_score += 0.2
        
        return {
            "has_issues": len(issues) > 0,
            "issues": issues,
            "sparsity_score": sparsity_score,
            "paragraph_count": len(paragraphs),
            "median_length": median_length
        }
    
    def verify_stereotype_content(self, content: str) -> Dict[str, Any]:
        """æ£€æŸ¥åˆ»æ¿å°è±¡æƒ…å†µï¼ˆåŸºäºstereotype_verifier.pyï¼‰"""
        
        # å®šä¹‰åˆ»æ¿å°è±¡è¯
        stereotype_words = [
            "é¦–å…ˆï¼Œ", "å…¶æ¬¡ï¼Œ", "å†æ¬¡ï¼Œ", "æœ€åï¼Œ", "å†è€…ï¼Œ",
            "ç»¼ä¸Šæ‰€è¿°ï¼Œ", "å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œ", "æ€»è€Œè¨€ä¹‹ï¼Œ", "æ¢å¥è¯è¯´ï¼Œ",
            "æ¯«æ— ç–‘é—®ï¼Œ", "æ˜¾è€Œæ˜“è§ï¼Œ", "ä¼—æ‰€å‘¨çŸ¥ï¼Œ"
        ]
        
        # åˆ†å‰²æ®µè½
        paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
        
        issues = []
        found_stereotype_words = []
        paragraphs_with_stereotype = 0
        
        for paragraph in paragraphs:
            has_stereotype = False
            
            # æ£€æµ‹å›ºå®šçš„åˆ»æ¿å°è±¡è¯
            for word in stereotype_words:
                if word in paragraph:
                    if not has_stereotype:
                        paragraphs_with_stereotype += 1
                        has_stereotype = True
                    if word not in found_stereotype_words:
                        found_stereotype_words.append(word)
            
            # æ£€æµ‹å„ç§åŠ ç²—æ ‡é¢˜æ¨¡å¼
            bold_patterns = [
                (r'^\*\*(.{1,15})\*\*[ï¼š:]', "æ®µè½ä»¥åŠ ç²—çŸ­è¯­å’Œå†’å·å¼€å¤´"),
                (r'\d+\.\s*\*\*(.{1,15})\*\*[ï¼š:]', "æ•°å­—åºå·+åŠ ç²—æ ‡é¢˜"),
                (r'^\s*-\s*\*\*(.{1,15})\*\*[ï¼š:]', "åˆ—è¡¨é¡¹+åŠ ç²—æ ‡é¢˜")
            ]
            
            for pattern, description in bold_patterns:
                if re.match(pattern, paragraph):
                    if not has_stereotype:
                        paragraphs_with_stereotype += 1
                        has_stereotype = True
                    if description not in found_stereotype_words:
                        found_stereotype_words.append(description)
                    break
        
        if found_stereotype_words:
            issues.append(f"å‘ç°åˆ»æ¿å°è±¡è¡¨è¾¾: {', '.join(found_stereotype_words)}")
        
        return {
            "has_issues": len(issues) > 0,
            "issues": issues,
            "found_expressions": found_stereotype_words,
            "affected_paragraphs": paragraphs_with_stereotype,
            "total_paragraphs": len(paragraphs)
        }
    
    def verify_latex_formulas(self, content: str) -> Dict[str, Any]:
        """æ£€æŸ¥LaTeXå…¬å¼æ ¼å¼"""
        
        issues = []
        
        # æ£€æŸ¥è£¸éœ²çš„å¸Œè…Šå­—æ¯
        greek_letters = [
            'Î±', 'Î²', 'Î³', 'Î´', 'Îµ', 'Î¶', 'Î·', 'Î¸', 'Î¹', 'Îº', 'Î»', 'Î¼', 'Î½', 'Î¾', 'Î¿', 'Ï€', 'Ï', 'Ïƒ', 'Ï„', 'Ï…', 'Ï†', 'Ï‡', 'Ïˆ', 'Ï‰',
            'Î‘', 'Î’', 'Î“', 'Î”', 'Î•', 'Î–', 'Î—', 'Î˜', 'Î™', 'Îš', 'Î›', 'Îœ', 'Î', 'Î', 'ÎŸ', 'Î ', 'Î¡', 'Î£', 'Î¤', 'Î¥', 'Î¦', 'Î§', 'Î¨', 'Î©'
        ]
        
        lines = content.split('\n')
        for i, line in enumerate(lines, 1):
            # è·³è¿‡å·²ç»åœ¨LaTeXå—ä¸­çš„å†…å®¹
            if '$' in line or '$$' in line:
                continue
            
            # æ’é™¤å›¾ç‰‡å¼•ç”¨çš„å†…å®¹ï¼ˆåŒ…æ‹¬alt textï¼‰
            # ç§»é™¤å›¾ç‰‡è¯­æ³• ![alt text](url)
            line_without_images = re.sub(r'!\[([^\]]*)\]\([^)]+\)', '', line)
                
            for letter in greek_letters:
                if letter in line_without_images:
                    issues.append(f"ç¬¬{i}è¡Œï¼šå‘ç°è£¸éœ²çš„å¸Œè…Šå­—æ¯ '{letter}'ï¼Œåº”ä½¿ç”¨LaTeXæ ¼å¼")
        
        # æ£€æŸ¥å¸¸è§æ•°å­¦ç¬¦å·
        math_symbols = ['âˆ‘', 'âˆ', 'âˆ«', 'âˆ', 'â‰¤', 'â‰¥', 'â‰ ', 'Â±', 'âˆ', 'âˆˆ', 'âˆ€', 'âˆƒ']
        for i, line in enumerate(lines, 1):
            if '$' in line or '$$' in line:
                continue
            
            # æ’é™¤å›¾ç‰‡å¼•ç”¨çš„å†…å®¹
            line_without_images = re.sub(r'!\[([^\]]*)\]\([^)]+\)', '', line)
                
            for symbol in math_symbols:
                if symbol in line_without_images:
                    issues.append(f"ç¬¬{i}è¡Œï¼šå‘ç°è£¸éœ²çš„æ•°å­¦ç¬¦å· '{symbol}'ï¼Œåº”ä½¿ç”¨LaTeXæ ¼å¼")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ•°å­¦è¡¨è¾¾å¼æ²¡æœ‰ç”¨LaTeXåŒ…å›´
        math_patterns = [
            r'\b[a-zA-Z]\s*=\s*[a-zA-Z0-9+\-*/^()]+',  # å˜é‡èµ‹å€¼
            r'\b[a-zA-Z]+\s*\^\s*[0-9]+',  # å¹‚è¿ç®—
            r'\b[a-zA-Z]+_[a-zA-Z0-9]+',  # ä¸‹æ ‡
        ]
        
        for i, line in enumerate(lines, 1):
            if '$' in line or '$$' in line:
                continue
            
            # æ’é™¤å›¾ç‰‡å¼•ç”¨çš„å†…å®¹
            line_without_images = re.sub(r'!\[([^\]]*)\]\([^)]+\)', '', line)
                
            for pattern in math_patterns:
                if re.search(pattern, line_without_images):
                    issues.append(f"ç¬¬{i}è¡Œï¼šå‘ç°å¯èƒ½çš„æ•°å­¦è¡¨è¾¾å¼ï¼Œå»ºè®®ä½¿ç”¨LaTeXæ ¼å¼åŒ…å›´")
                    break
        
        return {
            "has_issues": len(issues) > 0,
            "issues": issues
        }
    
    def verify_citations(self, content: str, bib_file_path: Optional[str] = None) -> Dict[str, Any]:
        """æ£€æŸ¥æ–‡çŒ®å¼•ç”¨æ ¼å¼å’Œå­˜åœ¨æ€§"""
        
        issues = []
        
        # æŸ¥æ‰¾æ‰€æœ‰å¼•ç”¨
        citation_pattern = r'\[@([^\]]+)\]'
        citations = re.findall(citation_pattern, content)
        
        if not citations:
            return {
                "has_issues": False,
                "issues": [],
                "citations_found": 0,
                "unique_citations": 0
            }
        
        # æ£€æŸ¥å¼•ç”¨æ ¼å¼ï¼Œå…ˆç§»é™¤å›¾ç‰‡å¼•ç”¨å†æ£€æŸ¥
        content_without_images = re.sub(r'!\[([^\]]*)\]\([^)]+\)', '', content)
        invalid_citation_pattern = r'\[(?!@)[^\]]*\]'
        invalid_citations = re.findall(invalid_citation_pattern, content_without_images)
        
        for invalid in invalid_citations:
            # æ’é™¤æ­£å¸¸çš„é“¾æ¥å’Œæ•°å­—å¼•ç”¨
            if not ('http' in invalid or invalid.isdigit()):
                issues.append(f"å‘ç°éæ ‡å‡†å¼•ç”¨æ ¼å¼: [{invalid}]ï¼Œåº”ä½¿ç”¨[@key]æ ¼å¼")
        
        # å¦‚æœæä¾›äº†bibæ–‡ä»¶ï¼Œæ£€æŸ¥å¼•ç”¨æ˜¯å¦å­˜åœ¨
        if bib_file_path and os.path.exists(bib_file_path):
            try:
                with open(bib_file_path, 'r', encoding='utf-8') as f:
                    bib_content = f.read()
                
                # ä½¿ç”¨æ—§ç‰ˆæœ¬API
                bib_db = bibtexparser.loads(bib_content)
                bib_keys = set(bib_db.entries_dict.keys())
                
                for citation in citations:
                    if citation not in bib_keys:
                        issues.append(f"å¼•ç”¨ [@{citation}] åœ¨bibæ–‡ä»¶ä¸­ä¸å­˜åœ¨")
                        
            except Exception as e:
                issues.append(f"è¯»å–bibæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
        
        return {
            "has_issues": len(issues) > 0,
            "issues": issues,
            "citations_found": len(citations),
            "unique_citations": len(set(citations))
        }
    
    def verify_images(self, content: str, md_file_path: str) -> Dict[str, Any]:
        """æ£€æŸ¥å›¾ç‰‡é“¾æ¥å’Œæ–‡ä»¶å­˜åœ¨æ€§"""
        
        issues = []
        
        # æŸ¥æ‰¾æ‰€æœ‰å›¾ç‰‡å¼•ç”¨
        image_pattern = r'!\[([^\]]*)\]\(([^)]+)\)'
        images = re.findall(image_pattern, content)
        
        md_dir = Path(md_file_path).parent
        
        for alt_text, img_path in images:
            # æ£€æŸ¥æ˜¯å¦ä¸ºç›¸å¯¹è·¯å¾„
            if img_path.startswith('http://') or img_path.startswith('https://'):
                issues.append(f"å›¾ç‰‡ '{alt_text}' ä½¿ç”¨äº†ç½‘ç»œé“¾æ¥ï¼Œåº”ä½¿ç”¨æœ¬åœ°ç»å¯¹è·¯å¾„: {img_path}")
                continue
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºç»å¯¹è·¯å¾„
            if not os.path.isabs(img_path):
                issues.append(f"å›¾ç‰‡ '{alt_text}' ä½¿ç”¨äº†ç›¸å¯¹è·¯å¾„ï¼Œåº”ä½¿ç”¨ç»å¯¹è·¯å¾„: {img_path}")
                continue
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(img_path):
                issues.append(f"å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {img_path}")
        
        return {
            "has_issues": len(issues) > 0,
            "issues": issues,
            "images_found": len(images)
        }
    
    def verify_code_blocks(self, content: str) -> Dict[str, Any]:
        """æ£€æŸ¥ä»£ç å—ï¼ˆä¸å…è®¸å­˜åœ¨ï¼‰"""
        
        issues = []
        
        # æ£€æŸ¥ä»£ç å—
        code_block_patterns = [
            (r'```[\s\S]*?```', "ä»£ç å—"),
            (r'`[^`\n]+`', "è¡Œå†…ä»£ç "),
            (r'^\s{4,}[^\s]', "ç¼©è¿›ä»£ç å—"),
        ]
        
        lines = content.split('\n')
        
        # æ£€æŸ¥å›´æ ä»£ç å—
        in_code_block = False
        for i, line in enumerate(lines, 1):
            if line.strip().startswith('```'):
                if not in_code_block:
                    in_code_block = True
                    block_type = line.strip()[3:].strip() or "ä»£ç "
                    issues.append(f"ç¬¬{i}è¡Œï¼šå‘ç°{block_type}ä»£ç å—ï¼Œè®ºæ–‡ä¸­ä¸åº”åŒ…å«ä»£ç å—")
                else:
                    in_code_block = False
        
        # æ£€æŸ¥è¡Œå†…ä»£ç 
        for i, line in enumerate(lines, 1):
            if '`' in line and not line.strip().startswith('```'):
                # ç®€å•æ£€æŸ¥æ˜¯å¦ä¸ºè¡Œå†…ä»£ç 
                backtick_count = line.count('`')
                if backtick_count >= 2:
                    issues.append(f"ç¬¬{i}è¡Œï¼šå‘ç°è¡Œå†…ä»£ç ï¼Œè®ºæ–‡ä¸­ä¸åº”åŒ…å«ä»£ç ")
        
        return {
            "has_issues": len(issues) > 0,
            "issues": issues
        }
    
    def verify_reference_count(self, content: str, min_references: int = 15) -> Dict[str, Any]:
        """æ£€æŸ¥å¼•ç”¨æ•°é‡æ˜¯å¦ç¬¦åˆå­¦æœ¯è®ºæ–‡æ ‡å‡†"""
        
        # æŸ¥æ‰¾æ‰€æœ‰å¼•ç”¨
        citation_pattern = r'\[@([^\]]+)\]'
        citations = re.findall(citation_pattern, content)
        unique_citations = len(set(citations))
        total_citations = len(citations)
        
        issues = []
        warnings = []
        
        # ç”Ÿæˆå¼•ç”¨ç»Ÿè®¡ä¿¡æ¯
        if unique_citations == 0:
            warnings.append("è®ºæ–‡ä¸­æœªå‘ç°ä»»ä½•æ–‡çŒ®å¼•ç”¨")
        elif unique_citations < min_references:
            warnings.append(f"è®ºæ–‡åŒ…å« {unique_citations} ä¸ªç‹¬ç«‹å¼•ç”¨ï¼Œå°‘äºå»ºè®®çš„ {min_references} ä¸ª")
        
        # æ·»åŠ å…·ä½“çš„å¼•ç”¨åˆ†æ
        if total_citations != unique_citations:
            duplicate_count = total_citations - unique_citations
            warnings.append(f"å‘ç° {duplicate_count} ä¸ªé‡å¤å¼•ç”¨")
        
        # æ³¨æ„ï¼šè¿™é‡Œä¸ç›´æ¥åˆ¤å®šä¸º"é”™è¯¯"ï¼Œè€Œæ˜¯æä¾›ä¿¡æ¯ä¾›LLMåˆ¤æ–­
        return {
            "has_issues": False,  # ä¸ç›´æ¥åˆ¤å®šä¸ºé”™è¯¯ï¼Œè®©LLMæ ¹æ®ä¸Šä¸‹æ–‡åˆ¤æ–­
            "issues": issues,
            "warnings": warnings,
            "unique_citations": unique_citations,
            "total_citations": total_citations,
            "meets_standard": unique_citations >= min_references,
            "min_expected": min_references,
            "suggestion": self._get_reference_count_suggestion(unique_citations, min_references)
        }
    
    def _get_reference_count_suggestion(self, unique_count: int, min_expected: int) -> str:
        """ç”Ÿæˆå¼•ç”¨æ•°é‡ç›¸å…³çš„å»ºè®®"""
        if unique_count == 0:
            return "è¯·æ£€æŸ¥å­¦ç”Ÿçš„ä»»åŠ¡è¦æ±‚æ˜¯å¦éœ€è¦åŒ…å«æ–‡çŒ®å¼•ç”¨ã€‚å¦‚æœæ˜¯å®Œæ•´å­¦æœ¯è®ºæ–‡ï¼Œå»ºè®®æ·»åŠ ç›¸å…³æ–‡çŒ®æ”¯æ’‘è§‚ç‚¹ã€‚"
        elif unique_count < min_expected:
            return (f"å½“å‰å¼•ç”¨æ•°é‡ ({unique_count}) ä½äºä¸€èˆ¬å­¦æœ¯è®ºæ–‡æ ‡å‡† ({min_expected})ã€‚"
                   f"è¯·ç¡®è®¤ï¼š1) æ˜¯å¦ä¸ºå®Œæ•´è®ºæ–‡ï¼Ÿ2) å­¦ç”Ÿæ˜¯å¦æœ‰ç‰¹æ®Šè¦æ±‚ï¼Ÿ3) è®ºæ–‡ç±»å‹æ˜¯å¦éœ€è¦å¤§é‡å¼•ç”¨ï¼Ÿ")
        else:
            return "å¼•ç”¨æ•°é‡ç¬¦åˆå­¦æœ¯è®ºæ–‡æ ‡å‡†ã€‚"

    def verify_bib_references(self, bib_file_path: str) -> Dict[str, Any]:
        """éªŒè¯bibæ–‡ä»¶ä¸­çš„å‚è€ƒæ–‡çŒ®çœŸå®æ€§"""
        
        if not os.path.exists(bib_file_path):
            return {
                "has_issues": True,
                "issues": [f"bibæ–‡ä»¶ä¸å­˜åœ¨: {bib_file_path}"],
                "verified_count": 0,
                "total_count": 0
            }
        
        issues = []
        verified_count = 0
        total_count = 0
        
        try:
            with open(bib_file_path, 'r', encoding='utf-8') as f:
                bib_content = f.read()
            
            # ä½¿ç”¨æ—§ç‰ˆæœ¬API
            bib_db = bibtexparser.loads(bib_content)
            entries = bib_db.entries
            
            for entry in entries:
                total_count += 1
                
                title = entry.get('title', '')
                author = entry.get('author', '')
                entry_key = entry.get('ID', '')
                
                if not title:
                    issues.append(f"å‚è€ƒæ–‡çŒ® {entry_key} ç¼ºå°‘æ ‡é¢˜")
                    continue
                
                # æœç´¢éªŒè¯
                search_result = self.web_search.search_reference(title, author)
                
                if not search_result["success"]:
                    issues.append(f"å‚è€ƒæ–‡çŒ® {entry_key} éªŒè¯å¤±è´¥: {search_result.get('error', 'Unknown error')}")
                elif not search_result["found"]:
                    issues.append(f"å‚è€ƒæ–‡çŒ® {entry_key} å¯èƒ½ä¸çœŸå®å­˜åœ¨: {title}")
                else:
                    verified_count += 1
                    
        except Exception as e:
            issues.append(f"è§£æbibæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
        
        return {
            "has_issues": len(issues) > 0,
            "issues": issues,
            "verified_count": verified_count,
            "total_count": total_count
        }


async def verify_paper(md_file_path: str, bib_file_path: Optional[str] = None, 
                      serper_api_key: Optional[str] = None) -> Dict[str, Any]:
    """
    ç»¼åˆéªŒè¯è®ºæ–‡
    
    Args:
        md_file_path: Markdownæ–‡ä»¶è·¯å¾„
        bib_file_path: bibæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        serper_api_key: Serper APIå¯†é’¥ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        éªŒè¯æŠ¥å‘Š
    """
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(md_file_path):
        return {
            "success": False,
            "error": f"Markdownæ–‡ä»¶ä¸å­˜åœ¨: {md_file_path}"
        }
    
    # è¯»å–æ–‡ä»¶å†…å®¹
    try:
        with open(md_file_path, 'r', encoding='utf-8') as f:
            content = f.read()
    except Exception as e:
        return {
            "success": False,
            "error": f"è¯»å–æ–‡ä»¶å¤±è´¥: {str(e)}"
        }
    
    verifier = PaperVerifier(serper_api_key)
    
    # æ‰§è¡Œå„é¡¹éªŒè¯
    results = {
        "success": True,
        "md_file_path": md_file_path,
        "bib_file_path": bib_file_path,
        "verification_results": {
            "sparse_content": verifier.verify_sparse_content(content),
            "stereotype_content": verifier.verify_stereotype_content(content),
            "latex_formulas": verifier.verify_latex_formulas(content),
            "citations": verifier.verify_citations(content, bib_file_path),
            "reference_count": verifier.verify_reference_count(content),
            "images": verifier.verify_images(content, md_file_path),
            "code_blocks": verifier.verify_code_blocks(content),
        }
    }
    
    # å¦‚æœæä¾›äº†bibæ–‡ä»¶ï¼ŒéªŒè¯å‚è€ƒæ–‡çŒ®
    if bib_file_path:
        results["verification_results"]["bib_references"] = verifier.verify_bib_references(bib_file_path)
    
    return results


def generate_report(verification_results: Dict[str, Any]) -> str:
    """ç”Ÿæˆè¯¦ç»†çš„éªŒè¯æŠ¥å‘Šï¼ˆMarkdownæ ¼å¼ï¼‰"""
    
    if not verification_results["success"]:
        return f"# è®ºæ–‡éªŒè¯æŠ¥å‘Š\n\n**é”™è¯¯**: {verification_results['error']}\n"
    
    md_file = verification_results["md_file_path"]
    bib_file = verification_results.get("bib_file_path", "æœªæä¾›")
    
    report = f"""# è®ºæ–‡éªŒè¯æŠ¥å‘Š

**æ–‡ä»¶è·¯å¾„**: {md_file}
**bibæ–‡ä»¶**: {bib_file}
**éªŒè¯æ—¶é—´**: {verification_results.get('timestamp', 'N/A')}

---

"""
    
    results = verification_results["verification_results"]
    
    # 1. ç½—åˆ—æƒ…å†µæ£€æŸ¥
    sparse = results["sparse_content"]
    report += "## 1. ç½—åˆ—æƒ…å†µæ£€æŸ¥\n\n"
    if sparse["has_issues"]:
        report += "**çŠ¶æ€**: âŒ å‘ç°é—®é¢˜\n\n"
        for issue in sparse["issues"]:
            report += f"- {issue}\n"
        report += f"\n**ç¨€ç–åº¦å¾—åˆ†**: {sparse['sparsity_score']:.2f}\n"
    else:
        report += "**çŠ¶æ€**: âœ… é€šè¿‡\n"
    report += f"**æ®µè½æ•°é‡**: {sparse['paragraph_count']}\n\n"
    
    # 2. åˆ»æ¿å°è±¡æ£€æŸ¥
    stereotype = results["stereotype_content"]
    report += "## 2. åˆ»æ¿å°è±¡æ£€æŸ¥\n\n"
    if stereotype["has_issues"]:
        report += "**çŠ¶æ€**: âŒ å‘ç°é—®é¢˜\n\n"
        for issue in stereotype["issues"]:
            report += f"- {issue}\n"
        report += f"\n**å—å½±å“æ®µè½**: {stereotype['affected_paragraphs']}/{stereotype['total_paragraphs']}\n"
    else:
        report += "**çŠ¶æ€**: âœ… é€šè¿‡\n"
    report += "\n"
    
    # 3. LaTeXå…¬å¼æ£€æŸ¥
    latex = results["latex_formulas"]
    report += "## 3. LaTeXå…¬å¼æ ¼å¼æ£€æŸ¥\n\n"
    if latex["has_issues"]:
        report += "**çŠ¶æ€**: âŒ å‘ç°é—®é¢˜\n\n"
        for issue in latex["issues"]:
            report += f"- {issue}\n"
    else:
        report += "**çŠ¶æ€**: âœ… é€šè¿‡\n"
    report += "\n"
    
    # 4. æ–‡çŒ®å¼•ç”¨æ£€æŸ¥
    citations = results["citations"]
    report += "## 4. æ–‡çŒ®å¼•ç”¨æ£€æŸ¥\n\n"
    if citations["has_issues"]:
        report += "**çŠ¶æ€**: âŒ å‘ç°é—®é¢˜\n\n"
        for issue in citations["issues"]:
            report += f"- {issue}\n"
    else:
        report += "**çŠ¶æ€**: âœ… é€šè¿‡\n"
    report += f"**å¼•ç”¨æ•°é‡**: {citations['citations_found']} (å”¯ä¸€: {citations['unique_citations']})\n\n"
    
    # 5. å¼•ç”¨æ•°é‡ç»Ÿè®¡
    ref_count = results["reference_count"]
    report += "## 5. å¼•ç”¨æ•°é‡ç»Ÿè®¡\n\n"
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    report += f"**å”¯ä¸€å¼•ç”¨æ•°é‡**: {ref_count['unique_citations']}\n"
    report += f"**æ€»å¼•ç”¨æ¬¡æ•°**: {ref_count['total_citations']}\n"
    report += f"**å»ºè®®æœ€å°‘å¼•ç”¨**: {ref_count['min_expected']}\n"
    report += f"**æ˜¯å¦è¾¾æ ‡**: {'âœ… æ˜¯' if ref_count['meets_standard'] else 'âš ï¸ å¦'}\n\n"
    
    # æ˜¾ç¤ºè­¦å‘Šå’Œå»ºè®®
    if ref_count["warnings"]:
        report += "**âš ï¸ æ³¨æ„äº‹é¡¹**:\n"
        for warning in ref_count["warnings"]:
            report += f"- {warning}\n"
        report += "\n"
    
    report += f"**ğŸ’¡ å»ºè®®**: {ref_count['suggestion']}\n\n"
    
    # 6. å›¾ç‰‡æ£€æŸ¥
    images = results["images"]
    report += "## 6. å›¾ç‰‡é“¾æ¥æ£€æŸ¥\n\n"
    if images["has_issues"]:
        report += "**çŠ¶æ€**: âŒ å‘ç°é—®é¢˜\n\n"
        for issue in images["issues"]:
            report += f"- {issue}\n"
    else:
        report += "**çŠ¶æ€**: âœ… é€šè¿‡\n"
    report += f"**å›¾ç‰‡æ•°é‡**: {images['images_found']}\n\n"
    
    # 7. ä»£ç å—æ£€æŸ¥
    code_blocks = results["code_blocks"]
    report += "## 7. ä»£ç å—æ£€æŸ¥\n\n"
    if code_blocks["has_issues"]:
        report += "**çŠ¶æ€**: âŒ å‘ç°é—®é¢˜\n\n"
        for issue in code_blocks["issues"]:
            report += f"- {issue}\n"
    else:
        report += "**çŠ¶æ€**: âœ… é€šè¿‡\n"
    report += "\n"
    
    # 8. bibæ–‡ä»¶éªŒè¯
    if "bib_references" in results:
        bib_refs = results["bib_references"]
        report += "## 8. bibæ–‡ä»¶å‚è€ƒæ–‡çŒ®éªŒè¯\n\n"
        if bib_refs["has_issues"]:
            report += "**çŠ¶æ€**: âŒ å‘ç°é—®é¢˜\n\n"
            for issue in bib_refs["issues"]:
                report += f"- {issue}\n"
        else:
            report += "**çŠ¶æ€**: âœ… é€šè¿‡\n"
        report += f"**éªŒè¯æˆåŠŸ**: {bib_refs['verified_count']}/{bib_refs['total_count']}\n\n"
    
    # æ€»ç»“
    all_results = [results[key] for key in results]
    total_issues = sum(1 for result in all_results if result.get("has_issues", False))
    
    report += "---\n\n## æ€»ç»“\n\n"
    if total_issues > 0:
        report += f"**æ€»ä½“çŠ¶æ€**: âŒ å‘ç° {total_issues} ç±»é—®é¢˜\n\n"
        report += "**å»ºè®®**: è¯·æ ¹æ®ä¸Šè¿°é—®é¢˜é€ä¸€ä¿®æ­£è®ºæ–‡å†…å®¹ã€‚\n"
    else:
        report += "**æ€»ä½“çŠ¶æ€**: âœ… æ‰€æœ‰æ£€æŸ¥é€šè¿‡\n\n"
        report += "**ç»“è®º**: è®ºæ–‡æ ¼å¼å’Œå†…å®¹ç¬¦åˆè¦æ±‚ã€‚\n"
    
    return report 